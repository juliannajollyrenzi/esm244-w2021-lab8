---
title: 'Clustering: K Means & Hierarchical'
author: "Julianna Renzi"
date: "2/28/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(here)
library(janitor)
library(palmerpenguins)

# packages for cluster analysis:
library(NbClust)
library(cluster)
library(factoextra)
library(dendextend)
library(ggdendro)
```

# K-means clustering

## Exploratory visualization

Could there be a way to cluster by species?

```{r}
# Bill length vs. depth exploratory:
ggplot(data = penguins) +
  geom_point(aes(x = bill_length_mm,
                 y = bill_depth_mm,
                 color = species,
                 shape = sex),
             size = 3, alpha = 0.7) +
  scale_color_manual(values = c("orange", "cyan4", "darkmagenta"))

```

```{r}
# or flipper length vs. body mass
ggplot(data = penguins) +
  geom_point(aes(x = flipper_length_mm,
                 y = body_mass_g,
                 color = species,
                 shape = sex),
             size = 3, alpha = 0.7) +
  scale_color_manual(values = c("orange", "cyan4", "darkmagenta"))
```

## Pick the number of clusters

`NbClust::NbClust()` function can help provide help picking the number of clusters. It runs 30 different indices and says how many think there should be X number of clusters. 

```{r}
number_est <- NbClust(penguins[3:6], # this is just the four structural size meansurement variables from penguins
                      min.nc = 2, # min number of clusters
                      max.nc = 10, # max number of clusters
                      method = "kmeans")
```
```{r}
# check out the results (first summary report)
number_est

# By these estimators, 2 is identified as the best number of clusters by the largest number of algorithms (8/30), but it probably still makes sense to keep our 3 clusters (for 3 spp) and see how it does
```

## Create a complete, scaled version of the data

We'll use 3 clusters, although there may be a case for 2 since Adelie + chinstrap are similar. We are going to do this with complete cases - in other words, for the variables we’re using to perform k-means clustering on penguins (bill length, bill depth, flipper length, body mass), we are dropping any observation (row) where any of those are missing. Keep in mind that this may not be the best option for every scenario - in other cases (e.g. when we have a large proportion of missingness), we may want to impute missing values instead.

```{r}
# drop rows where there are missing values (in any of the cols)
penguins_complete <- penguins %>% 
  drop_na(bill_length_mm, bill_depth_mm, body_mass_g, flipper_length_mm)

# only keep the columns for the size measurements, then SCALE them
penguins_scale <- penguins_complete %>% 
  select(ends_with("mm"), body_mass_g) %>% 
  scale() # If scale is TRUE then scaling is done by dividing the (centered) columns of x by their standard deviations if center is TRUE, and the root mean square otherwise. If scale is FALSE, no scaling is done.
```

## Run k-means

```{r}
penguins_km <- kmeans(penguins_scale, 
                      3) # specify 3 groups to start

# can see the output:
penguins_km$size # how many observations are assigned to each cluster

penguins_km$cluster # what clusters each observation in penguins_scale is assigned to 
```
```{r}
# bind the cluster number to the original data used for clustering, so that we can see what cluster each penguin is assigned to
penguins_cl <- data.frame(penguins_complete, cluster_no = factor(penguins_km$cluster))

# plot flipper length versus body mass and visualize clusters
ggplot(data = penguins_cl) +
  geom_point(aes(x = flipper_length_mm,
                 y = body_mass_g,
                 color = cluster_no,
                 shape = species))
```
```{r}
# try the same braph but versus bill dimensions and mapping species and cluster number to the point shape/color aesthetics 
ggplot(data = penguins_cl) +
  geom_point(aes(x = bill_depth_mm,
                 y = bill_length_mm,
                 color = cluster_no,
                 shape = species))
```

We see that a lot of gentoos are in Cluster 3, a lot of Adelies are in Cluster 2, and A lot of chinstraps are in Cluster 1…but what are the actual counts? Let’s find them:

```{r}
# find the counts of each species assigned to each cluster, then pivot_wider() to make it a contingency table:
penguins_cl %>% 
  count(species, cluster_no) %>% 
  pivot_wider(names_from = cluster_no, values_from = n) %>%
  rename('Cluster 1' = '1', 'Cluster 2' = '2', 'Cluster 3' = '3')
```

Takeaway: as we see from the graph, most chinstraps in Cluster 1, and most Adelies in Cluster 2, and all Gentoos are in Cluster 3 by k-means clustering. So this actually does a somewhat decent job of splitting up the three species into different clusters, with some overlap in Cluster 1 between Adelies & chinstraps, which is consistent with what we observed in exploratory data visualization.

# Hierarchical clustering

We will use the `stats::hclust()` function for agglomerative hierarchical clustering, using WorldBank environmental data (simplified), wb_env.csv.

```{r}
# bring in the data
wb_env <- read_csv("wb_env.csv")

# only keep top 20 greenhouse gas emitters
wb_ghg_20 <- wb_env %>% 
  arrange(-ghg) %>% 
  head(20)
```

## Scale the data

```{r}
# only numeric columns
wb_scaled <- wb_ghg_20 %>% 
  select(3:7) %>% 
  scale()

# update to add rownames (country names from wb_ghg_20)
rownames(wb_scaled) <- wb_ghg_20$name # useful for visualizing
```

## Find Euclidean distances

Use the `stats::dist()` function to find the Euclidean distance in multivariate space between the different observations (countries):

```{r}
# compute dissimilarity values (Euclidean distances):
euc_distance <- dist(wb_scaled, method = "euclidean")

# this creates a 1 column result (why not a matrix??)
```

## Complete linkage with hclust()

The `stats::hclust()` function performs hierarchical clustering, given a dissimilarity matrix (our matrix of euclidean distances), using a linkage that you specify.

Here, let’s use complete linkage (recall from lecture: clusters are merged by the smallest maximum distance between two observations in distinct clusters).

```{r}
# Hierarchical clustering (complete linkage)
hc_complete <- hclust(euc_distance, method = "complete")

# plot it
plot(hc_complete, cex = 0.6, hang = -1)
# hang = The fraction of the plot height by which labels should hang below the rest of the plot. A negative value will cause the labels to hang down from 0.
```

## By single linkage

Let’s update the linkage to single linkage (recall from lecture: this means that clusters are merged by the smallest distance between observations in separate clusters):

```{r}
# single linking hierarchical clustering
hc_single <- hclust(euc_distance, method = "single")

plot(hc_single, cex = 0.6, hang = -1)
```

## Make a tanglegram

Let’s make a tanglegram to compare clustering by complete and single linkage! We’ll use the `dendextend::tanglegram()` function to make it.

First, we’ll convert to class `dendrogram`, then combine them into a list:

```{r}
# convert to class dendrogram
dend_complete <- as.dendrogram(hc_complete)
dend_simple <- as.dendrogram(hc_single)
```

Make a tanglegram

```{r}
tanglegram(dend_complete, dend_simple)
```
That allows us to compare how things are clustered by the different linkages!

## Use ggplot to plot

Here’s how you can make your dendrogram with ggplot (here, I’ll use the complete linkage example stored as hc_complete) using ggdendrogram(), a ggplot wrapper:

```{r}
ggdendrogram(hc_complete,
             rotate = TRUE) +
  theme_minimal() +
  labs(x = "Country")
```







